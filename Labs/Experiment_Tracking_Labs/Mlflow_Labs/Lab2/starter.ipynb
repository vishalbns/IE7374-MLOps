{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6203963d-fe11-4702-8be5-3b73523166a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    " \n",
    "white_wine = pd.read_csv(\"data/winequality-white.csv\", sep=\";\")\n",
    "red_wine = pd.read_csv(\"data/winequality-red.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835e86f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "white_wine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c83cf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "red_wine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "65fb5aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "red_wine['is_red'] = 1\n",
    "white_wine['is_red'] = 0\n",
    " \n",
    "data = pd.concat([red_wine, white_wine], axis=0)\n",
    " \n",
    "# Remove spaces from column names\n",
    "data.rename(columns=lambda x: x.replace(' ', '_'), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bed95c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc6ce03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.distplot(data.quality, kde=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47920a9",
   "metadata": {},
   "source": [
    "Looks like quality scores are normally distributed between 3 and 9.\n",
    "\n",
    "Define a wine as high quality if it has quality >= 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e4bf04aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_quality = (data.quality >= 7).astype(int)\n",
    "data.quality = high_quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f750cc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    " \n",
    "dims = (3, 4)\n",
    " \n",
    "f, axes = plt.subplots(dims[0], dims[1], figsize=(25, 15))\n",
    "axis_i, axis_j = 0, 0\n",
    "for col in data.columns:\n",
    "  if col == 'is_red' or col == 'quality':\n",
    "    continue # Box plots cannot be used on indicator variables\n",
    "  sns.boxplot(x=high_quality, y=data[col], ax=axes[axis_i, axis_j])\n",
    "  axis_j += 1\n",
    "  if axis_j == dims[1]:\n",
    "    axis_i += 1\n",
    "    axis_j = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7815f5f",
   "metadata": {},
   "source": [
    "In the above box plots, a few variables stand out as good univariate predictors of quality.\n",
    "\n",
    "In the alcohol box plot, the median alcohol content of high quality wines is greater than even the 75th quantile of low quality wines. High alcohol content is correlated with quality.\n",
    "In the density box plot, low quality wines have a greater density than high quality wines. Density is inversely correlated with quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503ebc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().any()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d461beaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    " \n",
    "X = data.drop([\"quality\"], axis=1)\n",
    "y = data.quality\n",
    " \n",
    "# Split out the training data\n",
    "X_train, X_rem, y_train, y_rem = train_test_split(X, y, train_size=0.6, random_state=123)\n",
    " \n",
    "# Split the remaining data equally into validation and test\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_rem, y_rem, test_size=0.5, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a731588",
   "metadata": {},
   "source": [
    "### Build a baseline model\n",
    "\n",
    "This task seems well suited to a random forest classifier, since the output is binary and there may be interactions between multiple variables.\n",
    "\n",
    "The following code builds a simple classifier using scikit-learn. It uses MLflow to keep track of the model accuracy, and to save the model for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c76d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.pyfunc\n",
    "import mlflow.sklearn\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from mlflow.models.signature import infer_signature\n",
    "from mlflow.utils.environment import _mlflow_conda_env\n",
    "import cloudpickle\n",
    "import time\n",
    " \n",
    "# The predict method of sklearn's RandomForestClassifier returns a binary classification (0 or 1). \n",
    "# The following code creates a wrapper function, SklearnModelWrapper, that uses \n",
    "# the predict_proba method to return the probability that the observation belongs to each class. \n",
    " \n",
    "class SklearnModelWrapper(mlflow.pyfunc.PythonModel):\n",
    "  def __init__(self, model):\n",
    "    self.model = model\n",
    "    \n",
    "  def predict(self, context, model_input):\n",
    "    return self.model.predict_proba(model_input)[:,1]\n",
    "\n",
    "\n",
    "# mlflow.start_run creates a new MLflow run to track the performance of this model. \n",
    "# Within the context, you call mlflow.log_param to keep track of the parameters used, and\n",
    "# mlflow.log_metric to record metrics like accuracy.\n",
    "with mlflow.start_run(run_name='untuned_random_forest'):\n",
    "    n_estimators = 10\n",
    "    model = RandomForestClassifier(n_estimators=n_estimators, random_state=np.random.RandomState(123))\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # predict_proba returns [prob_negative, prob_positive], so slice the output with [:, 1]\n",
    "    predictions_test = model.predict_proba(X_test)[:,1]\n",
    "    auc_score = roc_auc_score(y_test, predictions_test)\n",
    "    mlflow.log_param('n_estimators', n_estimators)\n",
    "    # Use the area under the ROC curve as a metric.\n",
    "    mlflow.log_metric('auc', auc_score)\n",
    "    wrappedModel = SklearnModelWrapper(model)\n",
    "    # Log the model with a signature that defines the schema of the model's inputs and outputs. \n",
    "    # When the model is deployed, this signature will be used to validate inputs.\n",
    "    signature = infer_signature(X_train, wrappedModel.predict(None, X_train))\n",
    "    \n",
    "    # MLflow contains utilities to create a conda environment used to serve models.\n",
    "    # The necessary dependencies are added to a conda.yaml file which is logged along with the model.\n",
    "    conda_env =  _mlflow_conda_env(\n",
    "            additional_conda_deps=None,\n",
    "            additional_pip_deps=[\"cloudpickle=={}\".format(cloudpickle.__version__), \"scikit-learn=={}\".format(sklearn.__version__)],\n",
    "            additional_conda_channels=None,\n",
    "        )\n",
    "    mlflow.pyfunc.log_model(\"random_forest_model\",\n",
    "                            python_model=wrappedModel,\n",
    "                            conda_env=conda_env,\n",
    "                            signature=signature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7f8d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = pd.DataFrame(model.feature_importances_, index=X_train.columns.tolist(), columns=['importance'])\n",
    "feature_importances.sort_values('importance', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6436ff02",
   "metadata": {},
   "source": [
    "As illustrated by the boxplots shown previously, both alcohol and density are important in predicting quality.\n",
    "\n",
    "You logged the Area Under the ROC Curve (AUC) to MLflow. Click Experiment at the upper right to display the Experiment Runs sidebar.\n",
    "\n",
    "The model achieved an AUC of 0.854.\n",
    "\n",
    "A random classifier would have an AUC of 0.5, and higher AUC values are better. For more information, see Receiver Operating Characteristic Curve.\n",
    "\n",
    "Register the model in MLflow Model Registry\n",
    "By registering this model in Model Registry, you can easily reference the model from anywhere within Databricks.\n",
    "\n",
    "The following section shows how to do this programmatically, but you can also register a model using the UI. See \"Create or register a model using the UI\" (AWS|Azure|GCP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f21fdff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = mlflow.search_runs(filter_string='tags.mlflow.runName = \"untuned_random_forest\"').iloc[0].run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5b06c9-9b68-4887-a0fe-2959e879c19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a6d9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you see the error \"PERMISSION_DENIED: User does not have any permission level assigned to the registered model\", \n",
    "# the cause may be that a model already exists with the name \"wine_quality\". Try using a different name.\n",
    "model_name = \"wine_quality\"\n",
    "model_version = mlflow.register_model(f\"runs:/{run_id}/random_forest_model\", model_name)\n",
    " \n",
    "# Registering the model takes a few seconds, so add a small delay\n",
    "time.sleep(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cb027b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The MlflowClient class allows you to interact with the MLflow Tracking Server programmatically. \n",
    "You can use it to perform various tasks, such as creating and managing experiments, starting \n",
    "and managing runs, logging metrics and parameters, and querying information about experiments and runs.\n",
    "\n",
    "\"\"\"\n",
    "from mlflow.tracking import MlflowClient\n",
    "client = MlflowClient()\n",
    "\n",
    "client.transition_model_version_stage(\n",
    "  name=model_name,\n",
    "  version=model_version.version,\n",
    "  stage=\"Production\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704bf9b3",
   "metadata": {},
   "source": [
    "The Models page now shows the model version in stage \"Production\".\n",
    "\n",
    "You can now refer to the model using the path \"models:/wine_quality/production\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344a6cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mlflow.pyfunc.load_model(f\"models:/{model_name}/production\")\n",
    " \n",
    "# Sanity-check: This should match the AUC logged by MLflow\n",
    "print(f'AUC: {roc_auc_score(y_test, model.predict(X_test))}')\n",
    "AUC: 0.8540300975814177"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dee7da",
   "metadata": {},
   "source": [
    "## Experiment with a new model\n",
    "\n",
    "The random forest model performed well even without hyperparameter tuning.\n",
    "\n",
    "The following code uses the xgboost library to train a more accurate model. It runs a parallel hyperparameter sweep to train multiple models in parallel, using Hyperopt and SparkTrials. As before, the code tracks the performance of each parameter configuration with MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7cc5a46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, SparkTrials, Trials, STATUS_OK\n",
    "from hyperopt.pyll import scope\n",
    "from math import exp\n",
    "import mlflow.xgboost\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    " \n",
    "search_space = {\n",
    "  'max_depth': scope.int(hp.quniform('max_depth', 4, 100, 1)),\n",
    "  'learning_rate': hp.loguniform('learning_rate', -3, 0),\n",
    "  'reg_alpha': hp.loguniform('reg_alpha', -5, -1),\n",
    "  'reg_lambda': hp.loguniform('reg_lambda', -6, -1),\n",
    "  'min_child_weight': hp.loguniform('min_child_weight', -1, 3),\n",
    "  'objective': 'binary:logistic',\n",
    "  'seed': 123, # Set a seed for deterministic training\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef9b3be",
   "metadata": {},
   "source": [
    "\n",
    "Nested Runs: When you use mlflow.start_run(nested=True) within the main run, you create nested runs. These runs are associated with the main run and inherit some of its parameters and context. Nested runs are typically used to explore variations or sub-experiments within the main experiment.\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import mlflow\n",
    "\n",
    "with mlflow.start_run():\n",
    "    # Your main experiment code goes here\n",
    "\n",
    "    with mlflow.start_run(nested=True):\n",
    "        # Nested experiment code goes here\n",
    "Parameters and metrics logged in the nested run are associated with that specific run and can be accessed separately from the main run.\n",
    "You can create multiple nested runs within a main run to represent different variations or configurations of your experiment.\n",
    "python\n",
    "Copy code\n",
    "import mlflow\n",
    "\n",
    "with mlflow.start_run():\n",
    "    # Your main experiment code goes here\n",
    "\n",
    "    with mlflow.start_run(nested=True):\n",
    "        # Nested experiment code 1 goes here\n",
    "\n",
    "    with mlflow.start_run(nested=True):\n",
    "        # Nested experiment code 2 goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e1b3f383",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(params):\n",
    "      # With MLflow autologging, hyperparameters and the trained model are automatically logged to MLflow.\n",
    "    mlflow.xgboost.autolog()\n",
    "    with mlflow.start_run(nested=True):\n",
    "        train = xgb.DMatrix(data=X_train, label=y_train)\n",
    "        validation = xgb.DMatrix(data=X_val, label=y_val)\n",
    "        # Pass in the validation set so xgb can track an evaluation metric. XGBoost terminates training when the evaluation metric\n",
    "        # is no longer improving.\n",
    "        booster = xgb.train(params=params, dtrain=train, num_boost_round=1000,\\\n",
    "                            evals=[(validation, \"validation\")], early_stopping_rounds=50)\n",
    "        validation_predictions = booster.predict(validation)\n",
    "        auc_score = roc_auc_score(y_val, validation_predictions)\n",
    "        mlflow.log_metric('auc', auc_score)\n",
    "\n",
    "        signature = infer_signature(X_train, booster.predict(train))\n",
    "        mlflow.xgboost.log_model(booster, \"model\", signature=signature)\n",
    "\n",
    "        # Set the loss to -1*auc_score so fmin maximizes the auc_score\n",
    "        return {'status': STATUS_OK, 'loss': -1*auc_score, 'booster': booster.attributes()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57266d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python3 -m pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b513b76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "conf_spark = SparkConf().set(\"spark.driver.host\", \"127.0.0.1\")\n",
    "sc = SparkContext(conf=conf_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6b79dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90f6f33",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Greater parallelism will lead to speedups, but a less optimal hyperparameter sweep. \n",
    "# A reasonable value for parallelism is the square root of max_evals.\n",
    "spark_trials = SparkTrials(parallelism=10)\n",
    "\n",
    "# Run fmin within an MLflow run context so that each hyperparameter configuration is logged as a child run of a parent\n",
    "# run called \"xgboost_models\" .\n",
    "with mlflow.start_run(run_name='xgboost_models'):\n",
    "  best_params = fmin(\n",
    "    fn=train_model, \n",
    "    space=search_space, \n",
    "    algo=tpe.suggest,\n",
    "    max_evals=96,\n",
    "    trials=spark_trials,\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c706e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run = mlflow.search_runs(order_by=['metrics.auc DESC']).iloc[0]\n",
    "print(f'AUC of Best Run: {best_run[\"metrics.auc\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f81372",
   "metadata": {},
   "source": [
    "## Update the production wine_quality model in MLflow Model Registry\n",
    "\n",
    "Earlier, you saved the baseline model to Model Registry with the name wine_quality. Now that you have a created a more accurate model, update wine_quality.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1062ad17",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model_version = mlflow.register_model(f\"runs:/{best_run.run_id}/model\", model_name)\n",
    " \n",
    "# Registering the model takes a few seconds, so add a small delay\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795a4239",
   "metadata": {},
   "source": [
    "Click Models in the left sidebar to see that the wine_quality model now has two versions.\n",
    "\n",
    "The following code promotes the new version to production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba884129",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.transition_model_version_stage(\n",
    "    name=model_name,\n",
    "    version=model_version.version,\n",
    "    stage='Archived'\n",
    ")\n",
    "\n",
    "client.transition_model_version_stage(\n",
    "    name=model_name,\n",
    "    version=new_model_version.version,\n",
    "    stage='Production'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a0a887",
   "metadata": {},
   "source": [
    "Clients that call load_model now receive the new model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31991db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mlflow.pyfunc.load_model(f\"models:/{model_name}/production\")\n",
    "print(f\"AUC: {roc_auc_score(y_test, model.predict(X_test))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5d7d35",
   "metadata": {},
   "source": [
    "## Batch inference\n",
    "\n",
    "There are many scenarios where you might want to evaluate a model on a corpus of new data. For example, you may have a fresh batch of data, or may need to compare the performance of two models on the same corpus of data.\n",
    "\n",
    "The following code evaluates the model on data stored in a Delta table, using Spark to run the computation in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a972228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import mlflow.pyfunc\n",
    "\n",
    "# apply_model_udf = mlflow.pyfunc.spark_udf(spark, f\"models:/{model_name}/production\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f3ec29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_data = spark.read.format(\"csv\").load(table_path) # table_path is path to the delta table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c1b95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e1b6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model_version.run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825fb547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serve the model using the MLflow Model Serving\n",
    "\n",
    "# Run the following in terminal outiside jupyter and after activating the virtual environment\n",
    "# mlflow models serve --env-manager=local -m models:/wine_quality/production -h 0.0.0.0 -p 5001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891e4814",
   "metadata": {},
   "source": [
    "- Here **model_name** is `wine_quality`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6e357e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = 'http://localhost:5001/invocations'\n",
    "\n",
    "datads_dict = {\"dataframe_split\": X_test.to_dict(orient='split')}\n",
    "\n",
    "response = requests.post(url, json=datads_dict)\n",
    "predictions = response.json()\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072da51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mlflow ui"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
